\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[]{algorithm2e}
\usepackage{pdfpages}
\usepackage{url}
%opening
\title{On Distant Supervision and its Application for Numerical Relation Extraction}
\author{}


\begin{document}
 
sg
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Bare Essentials}
This report will demand that the reader is on the same page vis-a-vis a few terminologies.
Trading off space and brevity for clarity:
a)Entity
b)Entity Pair
c)Relation
d)Mention
e)Match
f)Extraction

\section{Problem}

The next few sections lay the foundation for discussion of our solution.
  
\section{Snowballing}
Let us motivate the idea by considering the following related problem:

Suppose we want to populate the repository of founders of companies, and all that we know
for a fact is that Elon Musk is the founder of SpaceX.
The problem can be divided into two parts, each of them rely on an intuition about how human 
beings form sentences in general.

\begin{itemize}
 
\item The first of them is given an entity pair, and a corpus of documents, find out all the sentences that
express a relation between the entity-pair.

Command line ninjas will quickly think of the following solution:
grep -i 'entity1' sentences|grep -i 'entity2'

The intuition behind this perhaps the most obvious solution is that \emph{a sentence
that houses both the entities can be expected to express a relation between them}.
A quick web search with the query ``entity1'' and ``entity2'' will show that this 
intuition is not out of the blue.

\item Sentence structure depends on the relation being expressed.
In verbose, if two sentences express the same relation, there will be (okay, there can be expected to be)
many \emph{features} that are similar in both of them. These include POS tags, words around the entities,
dependency path between the entities to name a few.

\end{itemize}
Putting together the intuitions above, we can solve the problem as follows:
Collect all the sentences which have SpaceX and Elon Musk in them, extract features 
from these sentences. Favor those features which repeat.
Now us this set of features to extract similar pairs from other sentences.
A fancier solution would be to re use the extractions to learn more features, and continuing the process 
till the point of diminishing returns.

This seemingly shaky method actually works [\ref{snowball}] and is popular by the name of snowballing.

\section{Distant Supervision: Snowball scaled up}
\label{ds}
\subsection{Introduction}
If the idea of Snowball looks convincing, Distant supervision should follow naturally.
Earlier, we considered only one relation for one entity pair. Scale the amount of both of these up
and we have distant supervision.
The basic setup is as follows:
\begin{itemize}

\item a) KB : A knowledge base consisting of facts. The facts are 3-tuples; the entities and the corresponding relation.
For example:
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Entity & Entity & Relation \\
\hline
Donald Knuth & Wisconsin & Born In\\
Srinivasa Ramanujan & Erode & Born In \\
Alan Turing & London & Born In \\
Alon Musk & SpaceX & Founder Of\\
\hline
\end{tabular}
\end{center}
has 4 different facts

\item b) Corpus
The repository of text where we expect to find the sentences that express facts that we know.
We need another repository, called the test set, where we will run our extractor to obtain new facts.
These two can be the same.
\end{itemize}

\subsection{Distant Supervision Assumption}
Every sentence that has an entity pair ($e_1$, $e_2$) expresses the relation which exists between ($e_1$, $e_2$).
\subsection{Matching}
We next need to align our knowledge base with the corpus. This process is also called matching.
  
\begin{algorithm}
 \KwData{Corpus C, Knowledge Base KB}
 \KwResult{Training data, D, A set of matches}
 Break C into a set of sentences, S\;
 \For{each sentence s in S}{
  let E = all entity pairs in s\;
  \For{each entity pair ($e_1$, $e_2$) in E}{
  \If{$\exists$ relation r in KB with r($e_1$, $e_2$)}{
    add s to D with label r
   } {
   
  }
  }
 }
 \caption{Distant Supervision}
\end{algorithm}


\subsection{Training}
Recall that obtaining the sentences which express a relation gives us training data, which 
we want to use to learn relation extractors, our goal.
There are several ways to achieve this, starting from the naive ways of training sentence 
level classifier extending to fancier graphical model based learning.

We briefly discuss the different training methods as we look at a survey of works on Distant Supervision
so far.

\section{Survey}

The first distant supervision paper came out in 1999.
Since then, almost every knob that could be twisted in the ds machinery, has been 
twisted. Different types of relations and different types of datasets will pose different challenges
of course, and this report deals with some of them.
\subsection{Constructing Biological Knowledge Bases by Extracting Information from Text Sources, Craven and Kumlien 1999}
The was the first work to use distant supervision for creating a repository of biological facts. 
They targeted 5 different relations between Proteins, Tissues, Cell-Types, Diseases, Pharmacologic-Agents and Subcellular-structure.
A naive bayes based simple relation extractor is first described. The extractor works in 2 steps:  A classifier trained on hand labeled data
first labels whether a sentence \emph{can} express a particular relation. If yes, the sentence is searched for a pair of one of the 5 types
depending on the label and the fact is added to the database.
The authors note that it took an expert 35 hours to hand label the corpus. This forms the basis of motivating distant supervision based 
methods.The paper also identifies many areas of improvements, like a pair of entities taking up multiple relations, on which subsequent work has been
built up.

The authors obtain an improvement of around 9 precision points with distant supervision.
It is possible that the following points contributed to the improvement in scores:
\begin{itemize}
 \item Constrained entity set. Proteins and Tissues won't just appear together without any possible relation. 
 \item The corpus used was very well aligned with the knowledge base.
\end{itemize}

\subsection{Distant Supervision for the Web, Mintz et. al 2009}
This work revived the interest of the community around the problem of distant supervision.
The major contributions of this paper can be listed as follows:
\begin{itemize}
 \item \textbf{Distant Supervision for the Web}
 Craven and Kumlien had a limited knowledge base and a limited corpus to align it with.
 As the web exploded, the knowledge bases that became available were of the magnitude of FreeBase, and the
 corpus that can be aligned with it was the web. Mintz et. al bought this fact to the spotlight and sparked 
 a series of works.
 
 \item \textbf{Features}
 They designed a set of rich features that are used by researchers to date. These features include dependency paths, POS tag sequences, word sequences to name a few.
 
\end{itemize}

\subsection{Learning 5000 relation extractors}
The number of relations typically used in the works were limited to the range of 30-40. 
This work targeted 5000 relations, which is a big jump. To fight sparsity that will arise 
in the training data due to a large number of classes, they also train a top level classifier
which decides which extractor should be used for a particular document. Not all the extractors 
are employed for a given document.

\subsection{Riedel et. al}
Distant supervision assumption does not really holds when the knowledge base is not well aligned with the corpus.
Another way of saying this would be that the corpus can be expected to consist of sentences that can host entity pairs
in a wide range of contexts, and the sentences may have nothing to do with the relations in which the entity pairs appear.
They take the example of the relation ``nationality''. A wide range of popular entities that are born in the country 
will be mentioned in the sentences that may have nothing to do with the fact that they were born in that country.
In such cases, it can be argued that the training data generated will be extremely noisy and will lead to poor extraction 
performance. Riedel et. al relax the distant supervision assumption.

\section{MultiR}
For our experiments
\section{Numerical Relations}
Numerical relations are much like the usual entity-entity relations, just more problematic.
What we call a \emph{numerical relation} will usually be called an attribute colloquially. 
For example, Camel and 215cm are related via the relation ``Average Height''.

\section{Numerical Relation extraction using MultiR}
The problem of numerical relation extraction can be easily moulded into the distant supervision framework.
Recall from section [\ref{ds}] that we need a corpus and a knowledge base to generate the training data for distant supervision.
We also need to fix on the set of relations that will be targeted. 

\subsection{Relations}
We selected the following 10 relations for our experiments:
 \begin{center}
\begin{tabular}{|l|l|}
\hline
Relation Name & Relation Code \\
\hline
Land area (sq. km)&AG.LND.TOTL.K2\\
Foreign direct investment, net (current US\$)&BN.KLT.DINV.CD\\
Goods exports (current US\$)&BX.GSR.MRCH.CD\\
Electricity production (kWh)&EG.ELC.PROD.KH\\
CO2 emissions (kt)&EN.ATM.CO2E.KT\\
Pump price for diesel fuel (US\$ per liter)&EP.PMP.DESL.CD\\
Inflation, consumer prices (annual \%)&FP.CPI.TOTL.ZG\\
Internet users (per 100 people)&IT.NET.USER.P2\\
GDP (current US\$)&NY.GDP.MKTP.CD\\
Life expectancy at birth, total (years)&SP.DYN.LE00.IN\\
Population (Total)&SP.POP.TOTL\\
\hline
\end{tabular}
\end{center}

The relations were selected keeping in mind the availability of sentences which potentially express these facts
along with difference in units.

\subsection{Knowledge base}

Our knowledge base was derived scrapped from \url{data.worldbank.org}.
It has 4371979 numerical facts about 249 countries ranging over 1281 different attributes. 
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Country id & Number & Relation \\
\hline
/m/04g5k&3126000130&EG.ELC.PROD.KH\\
/m/02k8k&1969.179&EN.ATM.CO2E.KT\\
/m/06nnj&332315&SP.POP.TOTL\\
/m/019rg5&55.020073&SP.DYN.LE00.IN\\
/m/05sb1&19974.148&EN.ATM.CO2E.KT\\
/m/05v8c&10000000000&EG.ELC.PROD.KH\\
/m/03spz&7639000100&EG.ELC.PROD.KH\\
/m/06vbd&44249.688&EN.ATM.CO2E.KT\\
/m/0d060g&51.3&IT.NET.USER.P2\\
/m/05qkp&62.298927&SP.DYN.LE00.IN\\
\hline
\end{tabular}
\end{center}

\section{L'homme propose, et Dieu dispose}
Our life would have been simpler if MultiR would do the desired, extract the numerical relations from the corpus provided
information about some of the numerical relations. But as it happens, it was not meant to be.
The model not doing well may mean several things. Low quality training data being one of the primary reasons.
Indeed, it turned out that vanilla distant supervision leads to an \textbf{unprecedented} amount of false positives, which are the root cause
of everything that went wrong in the distant supervision pipeline.

\subsection{Numbers and False positives}
Why would numbers lead to false positives in the first place?
The problem stems from the fact that numbers have no identity of their own; they represent count of
some real entity or phenomenon.

\begin{itemize}
 \item \textbf{Numbers can appear in many more contexts with an entity}
The number of ways in which any two entities can appear together
in a sentence is far less than the number of ways in which a number and a quantity can appear
together. For example, Consider the entity pair “Bill Gates” and “Microsoft” and the entity-number
pair “Bill Gates” and “3” (say). While former will usually co-occur in finite contexts (Founder,
CEO, Evangelist etc.), the latter may co-occur anywhere Bill Gates happen to be around something
which is 3, the number of cars, billion dollars donated, number of units headed, position in the
company, number of business units shutdown by Microsoft and so on.
\item The situation is worse for smaller whole numbers, which are more frequent. This intuitively makes
sense as we are more often see 2,3 or 11 than 111212233 or 11.42143.
\item \textbf{The match mines}

During the initial phases of our experiments, we stumbled upon the \emph{match mines}
These were basically huge tables, world cup scores of all the matches played and so on.
A couple of such sentences were responsible for 21\% of the matches!
It is easy (and very important) to get rid of such sentences. For subsequent runs, we first sort the sentences by length and then remove
top 1000 of them.
\end{itemize}

The first point of debugging a model which is not performing as well as it should is doing 
sanity check on the training data. 

\section{Fighting False Positives with units}
Analyzing results of plain matching made it clear that units will help in improving both precision
(by eliminating matches where the unit is not present) and recall (increasing matches by
canonicalization of numbers and conversion to SI units). We found that though units helped in
drastically cutting down the number of false positives (match mines were completely eliminated),
and helped recall (lots of good matches for Land area and Population), the number of false positives
was still a trouble. The number of false positives was typically high for cases where the unit was
percentage, since it is again a very generic unit. For other relations too, the number of false
positives was very large. The large number of false positives, apart from degrading quality of the
model, make evaluating the quality of matcher very difficult.

\section{Numbers are weak entities: a case for keywords}

\subsection{Motivation}
Matches from unit extraction showed that in some cases, the sentence that supposedly labeled as a
match for a particular relation has no mention of the relation itself at all.
For example, consider:
“In eurozone powerhouse Germany, industrial orders jumped 3.2 percent in June, official data
showed Thursday, with foreign demand behind a sharp rebound following a surprise drop in May.”
In this sentence, (Germany, 3.2) was considered as a match pair for the relation Internet user
percent. Clearly, it has nothing to do with it.

\subsection{Numerical Relations are Explicit}
A key observation that can be made by going through the sentences that express numerical relations
is that one cannot be too poetic while forming a sentence that is supposed to state a numerical fact.
This is in stark contrast with sentences expressing relations between entity pairs, wherein the
underlying relation might be implicit. If we want to state GDP of a country in a sentence, there is no
escape from the words like “GPD” or “gross domestic product” and the likes * .

Compare this with a sentence that must relate Microsoft and Bill Gates. A few ways of stating that
Microsoft was founded by Bill Gates can be enumerated as follows:
\begin{itemize}
\item Bill Gates is the founder of Microsoft
\item Bill Gates founded Microsoft
\item Bill Gates is the father of Microsoft
\item Bill Gates laid the foundation stone of Microsoft
\item Bill Gates started Microsoft
\end{itemize}

If this is indeed true, imposing an additional constraint of keyword being present in a sentence in
addition to the fact being present can help in cutting down the number of false positives. We note
that such a pruning is possible only in case of numerical relations. As mentioned earlier, for real
world entity pairs, co-incidental matches will be rarer and a constraint on the relation word being
present will be too restrictive.
\subsection{Approach}
Let $M_r$ be the set of matches obtained by standard unit + distance based matching for a relation r.
We prune $M_r$ by picking only sentences which contain one of the words in the set keywords(r).
The sets keywords(r) are manually crafted.

\begin{center}
 \begin{tabular}{|l|l|}
\hline \\
 Relation Keywords & (case insensitive)\\
\hline
 Internet User \% &"Internet"\\
Land Area &"area", "land", "land area"\\
Population &"Population"\\
Diesel &"diesel"\\
GDP &"Gross domestic", "GDP"\\
CO2 &"Carbon", "Carbon Emission", "CO2"\\
Inflation &"Inflation", "Price Rise"\\
FDI &"Foreign", "FDI"\\
Goods Export & "goods"\\
Life Expectancy & "life", "life expectancy"\\
Electricity Production & "Electricity\\
\hline
\end{tabular}
 \end{center}



\section{Results}

\section{Possibilities}

\section{Rule Based Extraction}
Analyzing a number of sentences expressing numerical relations lead to several insights as already discussed.

\begin{itemize}
 \item \textbf{Keywords} Sentences expressing numerical relations can be expected to be explicit about the relation being expressed.
 Stated another way, we can expect presence of certain keywords that might help in identifying relations.
 \item \textbf{Modifiers} A large number of false positives stem out of mentions where a change in the numerical attribute is mentioned.
\end{itemize}

\subsection{Dependency Path}
We define a dependency path between two words ``A'' and ``B'' as the shortest path between them in the dependency graph.
The dependency graph consists of one node for each of the words, and the dependencies are the collapsed typed dependencies 
as obtained from the stanford dependency parser. [\ref{corenlp}]. 
The following figure shows dependency graph for ``The estimated 2014 population of Zambia is 15,200,000, which ranks 70th in the world.''
\begin{figure*}
 \centering
 \includegraphics[bb=0 0 990 149, scale = 0.4]{./dep.png}
 % dep.png: 990x149 pixel, 72dpi, 34.92x5.26 cm, bb=0 0 990 149
 \caption{Dependency Graph}
 \label{fig:1}
\end{figure*}


\subsection{Relation extraction using dependency paths}
Intuitively, it makes sense that the entities which are related will have some dependence on each other in the sentence 
expressing the relation.

\subsection{Extraction Algorithm}
\begin{algorithm}
 \KwData{Corpus C, KW(r) : Set of keywords for the relation r, Modifiers : A set of modifying words}
 \KwResult{A set of extractions of the form r(C, N) where C and N are a country and number respectively related via a relation r}
 Break C into a set of sentences, S\;
 \For{each sentence s in S}{
  let E = all entity pairs in s\;
  \For{each country-number pair ($C$, $N$) in E}{
      extract P, the dependency path between $C$ and $N$ \
      \For{relation r}{
	keywordCondition = $\exists$ keyword kw $\in$ KW(r) such that kw appears in P \
	modifierCondition = $\exists$ modifying word mw $\in$ Modifiers such that mw appears in P \;
	\If{keywordCondition $\and$ !modifierCondition} {\
	  print r(C, N)
	}
      } 
  }
}

\caption{Rule Based Relation Extraction}
\end{algorithm}

\subsection{Example}
Consider the Sentence ``Germany's population was counted to be 80,219,695 on May 9, 2011, making it the 16th most populous country in the world.''
The dependency parse graph for this sentence is as shown in [\ref{fig:2}]
\begin{figure}[h]
 \centering
 \includegraphics[bb=0 0 1245 177,scale=0.3]{./dep4.png}
 % dep2.png: 1245x177 pixel, 72dpi, 43.92x6.24 cm, bb=0 0 1245 177
 \label{fig:2}
\end{figure}

\subsection{Results}
\subsubsection{Precision and Recall}
The extractor was applied to 30 sentences expressing 23 different relations.
\begin{tabular}{|l|l|l|}
\hline
& Relations Present & Relations not Present (False positives) \\
\hline
Extracted & 16 & 17 \\
\hline
Not Extracted & 7 & N/A \\
\hline
\end{tabular}
 \begin{itemize}
  \item Precision: 48.4\%
  \item Recall: 69.6\%
 \end{itemize}

The precision will increase further on applying unit based pruning.

\subsubsection{Example Extractions}
\begin{tabular}{|l|} 
\hline
Sentence $\rightarrow$ Extraction \\
\hline
At 3.71 million square miles (9.62 million km2) and with around 318 million people,\\ 
the US is the world's 3rd or 4th-largest country by total area and third-largest by population. \\$\rightarrow$ POP( US, 318) \\
\hline
The land area of the contiguous US is 2,959,064 square miles (7,663,941 km2)\\ $\rightarrow$  AGL( US, 2,959,064), AGL( US, 7,663,941) \\
\hline
With 1,210,193,422 residents reported in the 2011 provisional census,\\ 
India is the world's second-most populous country.\\\ $\rightarrow$ POP( India, 1,028,737,436)\\ 
\hline
According to an official estimate for 1 June 2014, the population of Russia is 143,800,000.\\ $\rightarrow$
POP( Russia, 1), POP( Russia, 2014), POP( Russia, 143,800,000) \\
\hline
\end{tabular}

\input{bib}
\end{document}
