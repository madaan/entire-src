\section{Introduction}
\subsection{Problem Statement}
The Internet is a web of mostly unstructured knowledge woven around things. However, these things; people, places, technologies, movies, products, books
etc. are mostly just mentioned by their name, with other crucial bits of information about them scattered around the point of mention. The cosmic scale of 
such unstructured information has stemmed the dream of a semantic web. A web which is aware of the links that make sense, which \emph{understands} what the 
user is looking for and which is gifted with the intelligence of locating the desideratum.

There are several pieces in the puzzle of the semantic web, this report is an attempt to understand one important piece; entities on the web and
their co occurrence statistics.


Given a knowledge base such as Yago or Freebase consisting of entities and relations, and the Web, our goal is 
to attach reliable estimates of the frequency of occurrences on the Web of various entities and relations as 
singletons, pairs (ordered and unordered) in a sentence.  
The aim is to collect statistics so as to be able to design prior probabilities about the set of 
entities and relations that can co-exist in a sentence or a paragraph. These statistics have applications in 
query interpretation and language understanding tasks.  
We can view it as being analogous to statistics in relational catalogs.

\subsection{Named Entity Recognition and Disambiguation}
For collecting the statistics of entities on the web, we need a method to determine which words in the 
free flowing interminable text are of interest, i.e. represent entities. 

Consider the following sentence : 

 \begin{center}
\textcolor{blue}{Michael Jordan is a Professor at Berkeley}
   \end{center}

 We first want to identify all the \textbf{named entities} in the text. The task is called named entity recognition and is 
 formally defined as : 
 \begin{mydef}[Named entity recognition\footnote {from \ref{thewiki}}]
 \label{nerdef}
   Named-entity recognition (NER) (also known as entity identification and entity extraction) is a subtask of information extraction that seeks to locate and classify 
   atomic elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
  \end{mydef}
 But we do not stop at that, we want to link each of the named entities thus recognized to a knowledge base\footnote{The knowledge base is a catalog of entities, like Wikipedia. Refer section [\ref{seckb}]}.
 Thus, our problem has a 2 step solution : 

 
 \begin{itemize}  
  \item Step 1 : \textbf{Identify} entities
  \medskip
  
  \textcolor{green}{Michael Jordan\_PERSON} is a professor at \textcolor{green}{Berkeley\_INSTITUTION} \medskip
  \item Step 2 : \textbf{Link} entities to knowledge bases : 
  \medskip
  
  \textcolor{red}{Michael Jordan\_ENTITY} (\url{http://en.wikipedia.org/wiki/Michael_I._Jordan})  is a professor at  
  \textcolor{red}{Berkeley\_ENTITY} (\url{http://en.wikipedia.org/wiki/University_of_California,_Berkeley})
\end{itemize}

The stanford NER library is a popular choice for recognizing named entities. [\ref{stanfordner}]
\subsubsection{Applications}


In simple terms, disambiguating named entities in the unstructured text imparts a structure to the document. 
We need two more data points to further appreciate the power that such a tool provides to us.
The first is the size of the web. As of 31st March 2014, there are atleast 1.8 billon indexed web pages.[\ref{ws}]
The second is the number of wikipedia entities. The wikipedia statistics [\ref{wikistats}] estimate the number of pages to be
around 32 million. Yago, a catalog of entities made from wikipedia has 12, 727, 222 entities.	
Imparting structure to documents at this magnitude has far reaching implications in the information
extraction and is a bridge towards the hitherto dream of a semantic web.  \\


It is highly recommended that the reader has a look at \url{http://www.google.co.in/insidesearch/features/search/knowledge.html}, 
The google knowledge graph project.


\subsubsection{Terminology}
The following terms are widely used in the literature on named entity disambiguation and thus in the survey.

\begin{itemize}
 \item \textbf{Mention, Spot} \\
 A piece of text which needs to be disambiguated. For example, \textbf{Amazon} has attracted a lot of visitors.
 \item \textbf{Entity} \\
 A named entity as defined in the definition \ref{nerdef}. 
 \item \textbf{Candidates} \\
 A set of entities which might be the correct disambiguation for a given mention.
 For example, possible candidates for the sentence above are Amazon river and Amazon.com
 \item \textbf{Prior} \\
 Probability of a mention linking to a particular entity. For example, the mention ``Amazon`` may be used
 to refer to the website (say) 60\% of the time.
 \item \textbf{Knowledge base} \\
 A catalog of Entities where an entity is as defined above. For example, Wikipedia or yago.

\end{itemize}

\subsection{A Baseline : Label and Collect}
The baseline which presents itself given the above problem is labeling the corpora with the named 
entities and then collecting the markings, keeping track of which entity was seen when along the way. 
As intuitive as it seems, the method is unlikely to perform well in the present scenario, owing to 
the mismatch in the training and test distribution [\ref{mmd}]. 
Our training data, hand labeled corpora, is paltry in comparison with the massive open web, where such 
systems are supposed to be deployed. This is true even for large training datasets like the Wikipedia.

The observation that we don't really want the individual labels is a first step towards a better solution. 
There are 3 reported methods for direct estimation of class ratios [\ref{mmd}]. We are interested in 
using one of them, maximum mean discrepancy for solving the problem in hand.

\subsection{Maximum mean discrepancy}
  

\subsection{Structure}
The report is divided into three parts.

\textbf{The first part} gives an overview of what are knowledge bases. This is important 
since the concept of such repositories of structured knowledge is central to the
report. 

\textbf{The second part} begins with an introduction to the problem of named entity disambiguation, the 
terminology and applications, and goes on to cover the techniques for named entity disambiguation
in some detail. We give and overview of the two broad categories of disambiguation techniques, Local and
global disambiguation.

\textbf{The third part} begins with a discussion on definition of Aggregate statistics and 
some of their applications. Finally, we discuss Maximum mean discrepancy and its 
application for estimating the aggregate statistics over entities.

