\chapter{Local Disambiguation of named entities}
\section{Introduction}
In local disambiguation, we collect just local evidences for each 
mention for its disambiguation. This was state of the art until the CSAW[\ref{thepaper}]
paper came along. We start by defining the problem and discussing the general form of solutions.
We then provide a short summary of approach followed in Wikify [\ref{wikify}] and the famous Milne and Witten paper [\ref{mw}]. A
solution based on machine learning[\ref{thepaper}] concludes the chapter.

\section{Problem definition}
We need to disambiguate a mention by collecting the local evidences. 
The evidences can be anything, POS tags, gender information, dictionary lookup 
etc. By local disambiguation, we mean that \textbf{we cannot use the disambiguation
information for any other entities for solving the problem.} 

\section{Solutions}

Every local disambiguation techniques fall into one of the following two categories[\ref{wikify}]

\begin{itemize}
 \item \textbf{Knowledge based} \\
 Derived from the classical word sense disambiguation literature, this 
 technique depends on the information drawn from the definitions provided by the knowledge base. 
 (See Lesk's algorithm [\ref{lesk}]).
 This is based on the overlap of context with the definitions of each of the candidate 
 senses as given in the knowledge base.
 
 \item \textbf{Machine Learning based} \\
 This method is based on collecting features from the mention and its surroundings, and
 training a classifier to give a verdict on a particular sense being a likely disambiguation
 of a mention. Machine learning based local disambiguation was almost unanimously adopted
 by the ned community as the solution for local disambiguation. AIDA changed the scene 
 by introducing a knowledge based local similarity score which works well.
 
 \end{itemize}

 \section{Related Work}

\subsection{Wikify[\ref{wikify}]}
The biggest contribution of this paper is perhaps presenting Wikipedia as the 
catalog against which were supposed to disambiguate. The paper also identifies
two broad methods of doing named entity disambiguation : Knowledge based and 
data based. Since the paper dates back to 2007, when the problem of NED was 
not as established, there are a lot of references to the problem of word disambiguation.

\subsection{Learning to link with Wikipedia[\ref{mw}]}
This paper defined three different features for disambiguation : 
\begin{itemize}
 \item Commonness  : This is the prior defined in Chapter 1.
 \item Relatedness : Perhaps the biggest contribution of this paper, the relatedness score,
 gives a measure for determining how similar the two entities are. This measure 
 is based on the number of common inlinks to entities in question.
 The relatedness measure as defined here has been used in a lot of works. In fact, all
 the approaches presented in the subsequent sections use this relatedness score, popular
 as the Milne-Witten score for finding out entity entity similarity.
 This score is defined as follows \\ \\
 $ r(\gamma, \gamma') = \frac{log|g(\gamma) \bigcap g(\gamma')| - log(max\{|g(\gamma)|, |(\gamma')|\})} {log c - log(min\{|g(\gamma)|, |(\gamma')|\})}$ 
 
 Where 
   \begin{itemize}
    \item $g(\gamma)$ : Set of wikipedia pages that link to $\gamma$
    \item $c :$ Total number of Wikipedia pages
    \item $r(\gamma, \gamma') :$ Relatedness of topics $\gamma$ and $\gamma'$
   \end{itemize}\bigskip

 
\end{itemize}
The algorithm selects a few unambiguous links in the document, and uses the similarity of the candidates
with these unambiguous links as a criteria for disambiguation.
Thus, in some sense, although the technique is not totally local, it shies away from doing anything to maintain
coherence among the entities that are unveiled and thus we do not call this method a ``Global method'', which 
are discussed in the following chapter.

\section{Machine learning based local disambiguation}
As mentioned, there are primarily two approaches for local disambiguation.
This section discusses a machine learning based local disambiguation method in some detail. This section
is based on the local disambiguation approach taken in [\ref{thepaper}].
\subsection{Definitions}
We first repeat the definitions for quick reference : 
\begin{itemize}
  \item $s$ : Spot, an Entity to be disambiguated (Christian leader John Paul) \bigskip 
  \item $\gamma$ : An entity label value (\url{http://en.wikipedia.org/wiki/Po-pe_John_Paul_II})  \bigskip 
 \item $f_s(\gamma)$ : A feature function that creates a vector of features given a spot and a candidate entity label.
 \end{itemize}
 
 \subsection{Local compatibility : Feature design} 
 The feature function takes the spot and the candidate as arguments. 
 
\begin{itemize} 
 
 \item The following information about a candidate $\gamma$ is used
\begin{itemize} 
 \item Text from the first descriptive paragraph of $\gamma$
  \item Text from the whole page for $\gamma$
  \item Anchor text within Wikipedia for $\gamma$.
  \item Anchor text and 5 tokens around $\gamma$ 
 \end{itemize}
 
 \item We now have 4 pieces of information about $\gamma$. We take each of these, and apply the following operations with 
 one argument as the spot
    \begin{itemize}
      \item{Dot-product between word count vectors}
      \item{Cosine similarity in TFIDF vector space}
      \item{Jaccard similarity between word sets}
  \end{itemize} 
  \end{itemize} 
 
 Thus, for a candidate - mention pair, we get a total of 12 Features (3 operations, 4 argument pairs). 
 
 In addition to these, we also use a sense probability prior as defined in the introduction. A popular way of 
 obtaining the prior is counting the number of times the spot has been linked to a particular entity. For example, 
 the hypertext ``Linux'' might be linked to the page for the Linux kernel 70\% of the times, and to the page
 for Linux based operating systems rest of the times.
 
 
\subsection{Compatibility Score}
Once we have the features, we train the classifier by using the following optimization objective : 
\begin{itemize}
 \item Local compatibility score between a spot $s$ and a candidate is given by $w^{T}f_s(\gamma)$
 \item $w$ is trained using an SVM like training objective
 \begin{center} $w^{T}f_s(\gamma) - w^{T}f_s(\gamma) \geq 1 - \epsilon_s$ \end{center}
 \end{itemize}
 
 \subsection{Finding the best candidate}

 \begin{algorithm}[H]
 \KwData{A Document d}
 \KwResult{Annotated document d' with every mention linked to the best candidate entity}
 \ForEach{mention $m$ in the document} {
  calculate $argmax_{c_m \in \Gamma}w^{T}f_m(c_m)$
  where $\Gamma$ = $c_m$ : $c_m$ is a possible disambiguation of $m$
 }\caption{Local disambiguation}
\end{algorithm}

 Note that a multi class classifier is not learned for several reasons, all of which can be mapped to 
 the large number of classes. 